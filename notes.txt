
SIMT
~~~~

Single Instruction Multiple Threads
The simplest way to understand SIMT is to imagine a multi-core system, where 
each core has its own register file, its own ALUs (both SIMD and Scalar) and its 
own data cache, but that unlike a standard multi-core system which has multiple 
independent instruction caches and decoders, as well as multiple independent 
Program Counter registers, the instructions are synchronously broadcast to all 
SIMT cores from a single unit with a single instruction cache and a single 
instruction decoder which reads instructions using a single Program Counter.

Since all Threads in a Warp execute the same instructions, branches result in 
extra code being unnecessarily executed.
  * For simple branches, predicated instructions are generated. These are not 
    terrible for performance.
  * For complex branches, Threads for the "other" conditional are flagged and 
    execute NOPs. This is the "branch divergence" penalty on CUDA.

SIMD
~~~~

Single Instruction Multiple Data

x64:
MMX (MultiMedia eXtensions)       64-bit %xmm0-%xmm7
SSE (Streaming SIMD Extensions)   128-bit %xmm0-%xmm7
SSE2                              128-bit %xmm0-%xmm15
AVX (Advanced Vector eXtensions)  256-bit %ymm0-%ymm15    
AVX-512                           512-bit %zmm0-%zmm31    EVEX prefix

Terms
~~~~~

Scalar: Single data type
Vector: Collection of scalar elements

DLA: Deep Learning Accelerator
ONNX: Open Neural Network Exchange
Frameworks:
  * PyTorch
  * TensorFlow
  * Caffe



Performance Toolkit
~~~~~~~~~~~~~~~~~~~

* Transformations (Vector Add, Matrix Multiply)
  - Grid Stride Loop
    Instead of starting a new thread, have the existing thread iterate over 
    the input vectors by Grid stride (gridDim * blockDim)
  - Shared Memory
    Use the faster Shared memory bank in each SP/Core to cache data common 
    to multiple threads. In Matrix Multiply use Tiling so that each thread 
    iterates over (common) rows and columns. If this is cached, the 32 threads 
    have reduced memory latency, and better thread occupancy.
* Reductions (Summing)
  - Binary Tree
    Each thread performs partial sum.
    . Use every thread for the first reduction [T0: 0+1; T1: 2+3; T2: 4+5;...]
    . Avoid using the % operator
    . Avoid Shared Memory Bank conflicts by [T0: 0+strids; T1: 1+(stride+1);...]
    . Grid Stride Loop to reduce problem to gridDim*blockDim
    . The load of global to shared memory does the first sum

Shared memory:
~~~~~~~~~~~~~~

* Faster than global memory
* Shared memory bank in each core (48K)
* Useful for Matrix Multiply type transformations since each thread does 
  iteration, and having a common cache speeds up memory access for all threads 
  in a Warp / Thread Block (32 threads).

  Static shared memory:

    const int size = 48;
    __global__ void k(...) {
      __shared__ int temp[size];
      ...
    }
    k<<<grid, block>>>(...);

  Dynamic shared memory:

    __global__ void k(...) {
      __shared__ int temp[];
      ...
    }
    int shared_size_in_bytes = 192;
    k<<<grid, block, shared_size_in_bytes>>>(...);

Unified Memory
~~~~~~~~~~~~~~

* cudaMallocManaged(&a, size);
* cudaMemPrefetchAsync(a, size, cudaGetDevice(&id));
* cudaMemPrefetchAsync(a, size, cudaCpuDeviceId);
* cudaFree(a);

Atomics
~~~~~~~

* Indivisible Read-modify-write; serialization
  o min/max
  o add/sub
  o inc/dec
  o and/or/xor
  o bitwise
  o exch/cas
* Uses:
  o Determine my place in an order
    int my_position = atomicAdd(order, 1);
  o Reserve space in a buffer
    int my_offset = atomicAdd(buffer_idx, my_dsize);

Vector Types
~~~~~~~~~~~~

These are vector types derived from the basic integer and floating-point types. 
They are structures and the 1st, 2nd, 3rd, and 4th components are accessible 
through the fields x, y, z, and w, respectively. They all come with a 
constructor function of the form make_<type name>; for example,

int2 make_int2(int x, int y);

BLAS
~~~~

Basic Linear Algebra Subprograms

Level 1: 
  * Vector, Vector operation
  * O(n)
  * axpy (a.x + y)
Level 2: 
  * Matrix, Vector operation
  * O(n^2)
  * GEMV (GEneral Matrix Vector multipy)
  * c = alpha*a*b' + c
Level 3: 
  * Matrix, Matrix operation
  * O(n^3)
  * GEMM (GEneral Matrix Multiply)
  * c = alpha*a*b + beta*c

GPU Architecture
~~~~~~~~~~~~~~~~

Software      Hardware
--------      --------
Thread        Scalar Processor (SP) (Core)
Thread Block  Multiprocessor (SM, Streaming Multiprocessor)
Grid          Device (GPU)

Nvidia machine code: SASS stored in ELF
Nvidia IR: PTX (similar to LLVM IR)

32 Threads per Warp
64 Warps per SM
64 to 192 SMs per GPU

1024 Threads per Block; Threads should be a multiple of Threads/Warp (32)
64K Blocks per Grid

Note: Shared memory & L1 cache are different memory structures

Architecture Generations
~~~~~~~~~~~~~~~~~~~~~~~~

* Tesla SM (CC1.0) (8 CUDA cores)
* Fermi SM (CC2.0) (32 CUDA cores)
* Kepler SMX (CC3.0) (192 CUDA cores)
* Maxwell SMM (CC5.0) (128 CUDA cores)
* Pascal (CC6.0) (64-128 CUDA cores)
* Volta (+ Tensor core) (CC7.0) (64 CUDA cores)
* Turing (CC7.5) (64 CUDA cores)
* Ampere (CC8.0) (64 CUDA cores)
* Hopper ()
* Lovelace (CC8.9)

GeForce GT 650M
~~~~~~~~~~~~~~~

* CC3.0
* GK107
* Kepler Architecture
* CUDA Cores 384 (SMs * SP-Cores/SM = 2 * 192)
* TMUs 32
* ROPs 16
* SMX Count 2
* Shared Memory 48 KB
* L1 Cache 16 KB (per SMX)
* L2 Cache 256 KB
* 729.6 GFLOPS

Quadro T2000
~~~~~~~~~~~~

* CC7.5
* TU117
* Turing Architecture
* CUDA Cores 1024 (SMs * SP-Cores/SM = 16 * 64)
* TMUs 64 (Texture Mapping Unit; rotate, resize, distort a bitmap image)
* ROPS 32 (Render Output Unit / Raster Operations Pipeline; final unit before framebuffer)
* Tensor Cores 0 (Tensor object; Matrix multiplication - gemm; fuzed multiply-add)
* RT Cores 0 (Real-Time Ray Tracing)
* Turing SM Count 16
* Shared Memory 32-64 KB
* L1 Cache 64-32 KB (per SM)
* L2 Cache 1 MB
* 3.656 TFLOPS

nvcc
~~~~

% nvcc -O2 --std=c++20 -v vectorAddGPU.cu -o vectorAddGPU
...
gcc vectorAdd.cu -o vectorAdd.cpp4.ii
cudafe++ --gen_c_file_name "vectorAdd.cudafe1.cpp" --stub_file_name "vectorAdd.cudafe1.stub.c" --gen_module_id_file --module_id_file_name "vectorAdd.module_id" "vectorAdd.cpp4.ii"
gcc vectorAdd.cu -o vectorAdd.cpp1.ii
cicc --include_file_name "vectorAdd.fatbin.c" -tused --module_id_file_name "vectorAdd.module_id" --gen_c_file_name "vectorAdd.cudafe1.c" --stub_file_name "vectorAdd.cudafe1.stub.c" --gen_device_file_name "vectorAdd.cudafe1.gpu"  "vectorAdd.cpp1.ii" -o "vectorAdd.ptx"
ptxas -arch=sm_52 -m64  "vectorAdd.ptx"  -o "vectorAdd.sm_52.cubin" 
fatbinary --create="vectorAdd.fatbin" "--image3=kind=elf,sm=52,file=vectorAdd.sm_52.cubin" "--image3=kind=ptx,sm=52,file=vectorAdd.ptx" --embedded-fatbin="vectorAdd.fatbin.c"
gcc vectorAdd.cudafe1.cpp -o vectorAdd.o
nvlink --register-link-binaries="vectorAdd_dlink.reg.c" "vectorAdd.o"  -lcudadevrt  -o "vectorAdd_dlink.sm_52.cubin" --host-ccbin "gcc"
fatbinary --create="vectorAdd_dlink.fatbin" " -link "--image3=kind=elf,sm=52,file=vectorAdd_dlink.sm_52.cubin" --embedded-fatbin="vectorAdd_dlink.fatbin.c" 
gcc -c -x c++ -DFATBINFILE="\"vectorAdd_dlink.fatbin.c\"" -DREGISTERLINKBINARYFILE="\"vectorAdd_dlink.reg.c\"" -o "vectorAdd_dlink.o" 
g++ "vectorAdd_dlink.o" "vectorAdd.o" -lcudadevrt -lcudart_static -lrt -lpthread -ldl -o "vectorAdd" 

% cuobjdump -lelf -ltext -lptx  vectorAdd.fatbin
ELF file    1: vectorAdd.1.sm_52.cubin
PTX file    1: vectorAdd.1.sm_52.ptx
% cuobjdump -lelf -ltext vectorAdd_dlink.fatbin
ELF file    1: vectorAdd_dlink.1.sm_52.cubin
% cuobjdump -lelf -ltext -lptx  vectorAdd
ELF file    1: vectorAdd.1.sm_52.cubin
ELF file    2: vectorAdd.2.sm_52.cubin
PTX file    1: vectorAdd.1.sm_52.ptx

% readelf -t vectorAdd.o | grep -A3 nv_fatbin
  [76] .nv_fatbin
       PROGBITS         0000000000000000  0000000000000cd0  0
       0000000000000b20 0000000000000000  0                 8
       [0000000000000002]: ALLOC
% readelf -t vectorAdd_dlink.o | grep -A3 nv_fatbin
  [ 5] .nv_fatbin
       PROGBITS         0000000000000000  0000000000000060  0
       00000000000003d0 0000000000000000  0                 8
       [0000000000000002]: ALLOC
% readelf -t vectorAdd | grep -A3 nv_fatbin
  [18] .nv_fatbin
       PROGBITS         0000000000091180  0000000000091180  0
       0000000000000ef0 0000000000000000  0                 8
       [0000000000000002]: ALLOC

% objdump -t vectorAdd | grep -i fat | c++filt 
00000000000ac1d8 l     O .bss 0000000000000008              __cudaFatCubinHandle
00000000000ac1e0 l     O .bss 0000000000000008              __cudaPrelinkedFatbins
0000000000091180 l       .nv_fatbin 0000000000000000              fatbinData
00000000000ac058 l     O .nvFatBinSegment 0000000000000018              __fatDeviceText
00000000000ac1f0 l     O .bss 0000000000000008              __nv_fatbinhandle_for_managed_rt
000000000000ac43 l     F .text  000000000000001a              __nv_save_fatbinhandle_for_managed_rt(void**)
00000000000ac208 l     O .bss 0000000000000008              __cudaFatCubinHandle
0000000000091550 l       .nv_fatbin 0000000000000000              fatbinData
00000000000ac070 l     O .nvFatBinSegment 0000000000000018              __fatDeviceText
000000000001ee20 l     F .text  0000000000000022              __cudaRegisterFatBinaryEnd
000000000001ee50 l     F .text  0000000000000037              __cudaUnregisterFatBinary
000000000001edf0 l     F .text  000000000000002b              __cudaRegisterFatBinary
00000000000ac088 g       .nvFatBinSegment 0000000000000000              _edata
00000000000ac058 g     O .nvFatBinSegment 0000000000000000              .hidden __TMC_END__

% nvcc --ptxas-options=-v vectorAdd.cu -o vectorAdd
ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function '_Z9vectorAddPKiS0_Pii' for 'sm_52'
ptxas info    : Function properties for _Z9vectorAddPKiS0_Pii
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 8 registers, 348 bytes cmem[0]

nvprof
~~~~~~

# List events & metrics
% nvprof --query-events
% nvprof --query-metrics

# Get *all* events/metrics
% sudo nvprof --events all ...
% sudo nvprof --metrics all ...

# Trace GPU
% sudo nvprof --print-gpu-trace ...

cuobjdump
~~~~~~~~~

% cuobjdump -sass vectorAddGPU

Fatbin elf code:
================
arch = sm_30
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit

  code for sm_30

Fatbin elf code:
================
arch = sm_30
code version = [1,7]
producer = cuda
host = linux
compile_size = 64bit

  code for sm_30
    Function : _Z9vectorAddPKfS0_Pfi
  .headerflags    @"EF_CUDA_SM30 EF_CUDA_PTX_SM(EF_CUDA_SM30)"
                                                                                /* 0x2202e2c282823307 */
        /*0008*/                   MOV R1, c[0x0][0x44];                        /* 0x2800400110005de4 */
        /*0010*/                   S2R R0, SR_CTAID.X;                          /* 0x2c00000094001c04 */
        /*0018*/                   S2R R3, SR_TID.X;                            /* 0x2c0000008400dc04 */
        /*0020*/                   IMAD R0, R0, c[0x0][0x28], R3;               /* 0x20064000a0001ca3 */
        /*0028*/                   ISETP.GE.AND P0, PT, R0, c[0x0][0x158], PT;  /* 0x1b0e40056001dc23 */
        /*0030*/               @P0 EXIT;                                        /* 0x80000000000001e7 */
        /*0038*/                   ISCADD R2.CC, R0, c[0x0][0x140], 0x2;        /* 0x4001400500009c43 */
                                                                                /* 0x22c04282c04282b7 */
        /*0048*/                   MOV32I R7, 0x4;                              /* 0x180000001001dde2 */
        /*0050*/                   IMAD.HI.X R3, R0, R7, c[0x0][0x144];         /* 0x208e80051000dce3 */
        /*0058*/                   ISCADD R4.CC, R0, c[0x0][0x148], 0x2;        /* 0x4001400520011c43 */
        /*0060*/                   LD.E R2, [R2];                               /* 0x8400000000209c85 */
        /*0068*/                   IMAD.HI.X R5, R0, R7, c[0x0][0x14c];         /* 0x208e800530015ce3 */
        /*0070*/                   LD.E R4, [R4];                               /* 0x8400000000411c85 */
        /*0078*/                   ISCADD R6.CC, R0, c[0x0][0x150], 0x2;        /* 0x4001400540019c43 */
                                                                                /* 0x20000002f04283f7 */
        /*0088*/                   IMAD.HI.X R7, R0, R7, c[0x0][0x154];         /* 0x208e80055001dce3 */
        /*0090*/                   FADD R0, R4, R2;                             /* 0x5000000008401c00 */
        /*0098*/                   ST.E [R6], R0;                               /* 0x9400000000601c85 */
        /*00a0*/                   EXIT;                                        /* 0x8000000000001de7 */
        /*00a8*/                   BRA 0xa8;                                    /* 0x4003ffffe0001de7 */
        /*00b0*/                   NOP;                                         /* 0x4000000000001de4 */
        /*00b8*/                   NOP;                                         /* 0x4000000000001de4 */
    ................................



Fatbin ptx code:
================
arch = sm_30
code version = [6,0]
producer = cuda
host = linux
compile_size = 64bit
compressed

% cuobjdump -lptx cudaTensorCoreGemm.simple
PTX file    1: cudaTensorCoreGemm.1.sm_90.ptx

% cuobjdump -arch sm_75 -sass cudaTensorCoreGemm.simple
...
Fatbin elf code:
================
arch = sm_75
code version = [1,7]
host = linux
compile_size = 64bit

        code for sm_75
                Function : _Z16simple_wmma_gemmP6__halfS0_PfS1_iiiff
        .headerflags    @"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM75 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM75)"
        /*0000*/                   IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] ;               /* 0x00000a00ff017624 */
                                                                                         /* 0x000fc400078e00ff */
        /*0010*/                   I2F.U32.RP R4, 0x20 ;                                 /* 0x0000002000047906 */
                                                                                         /* 0x000e220000209000 */
        /*0020*/                   S2R R5, SR_CTAID.X ;                                  /* 0x0000000000057919 */
                                                                                         /* 0x000e620000002500 */
        /*0030*/                   ISETP.LT.AND P2, PT, RZ, c[0x0][0x188], PT ;          /* 0x00006200ff007a0c */
                                                                                         /* 0x000fc60003f41270 */
        /*0040*/                   S2R R6, SR_TID.X ;                                    /* 0x0000000000067919 */
                                                                                         /* 0x000e660000002100 */
...

% cuobjdump -arch sm_75 -sass cudaTensorCoreGemm.simple | grep Function
    Function : _Z16simple_wmma_gemmP6__halfS0_PfS1_iiiff
    Function : _Z12compute_gemmPK6__halfS1_PKfPfff

% cuobjdump -arch sm_75 -sass cudaTensorCoreGemm.simple | grep Function | cu++filt 
    Function : simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float)
    Function : compute_gemm(const __half *, const __half *, const float *, float *, float, float)

% cuobjdump -ltext cudaTensorCoreGemm.simple | cu++filt 
SASS text section 1 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_70.elf.bin
SASS text section 2 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_70.elf.bin
SASS text section 3 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_75.elf.bin
SASS text section 4 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_75.elf.bin
SASS text section 5 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_80.elf.bin
SASS text section 6 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_80.elf.bin
SASS text section 7 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_86.elf.bin
SASS text section 8 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_86.elf.bin
SASS text section 9 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_89.elf.bin
SASS text section 10 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_89.elf.bin
SASS text section 11 : x-simple_wmma_gemm(__half *, __half *, float *, float *, int, int, int, float, float).sm_90.elf.bin
SASS text section 12 : x-compute_gemm(const __half *, const __half *, const float *, float *, float, float).sm_90.elf.bin

% cuobjdump -lptx cudaTensorCoreGemm.simple
PTX file    1: cudaTensorCoreGemm.1.sm_90.ptx

% cuobjdump -lelf cudaTensorCoreGemm.simple
ELF file    1: cudaTensorCoreGemm.1.sm_70.cubin
ELF file    2: cudaTensorCoreGemm.2.sm_75.cubin
ELF file    3: cudaTensorCoreGemm.3.sm_80.cubin
ELF file    4: cudaTensorCoreGemm.4.sm_86.cubin
ELF file    5: cudaTensorCoreGemm.5.sm_89.cubin
ELF file    6: cudaTensorCoreGemm.6.sm_90.cubin
ELF file    7: cudaTensorCoreGemm.7.sm_70.cubin
ELF file    8: cudaTensorCoreGemm.8.sm_75.cubin
ELF file    9: cudaTensorCoreGemm.9.sm_80.cubin
ELF file   10: cudaTensorCoreGemm.10.sm_86.cubin
ELF file   11: cudaTensorCoreGemm.11.sm_89.cubin
ELF file   12: cudaTensorCoreGemm.12.sm_90.cubin

% cuobjdump -ptx cudaTensorCoreGemm.simple
...
Fatbin ptx code:
================
arch = sm_90
code version = [8,2]
host = linux
compile_size = 64bit
compressed
ptxasOptions =  -maxrregcount=255

.version 8.2
.target sm_90
.address_size 64


.extern .shared .align 16 .b8 shmem[];

.visible .entry _Z12compute_gemmPK6__halfS1_PKfPfff(
.param .u64 _Z12compute_gemmPK6__halfS1_PKfPfff_param_0,
.param .u64 _Z12compute_gemmPK6__halfS1_PKfPfff_param_1,
...
)
{
.reg .pred %p<5>;
.reg .f32 %f<16581>;
.reg .b32 %r<16730>;
.reg .b64 %rd<1586>;

ld.param.u64 %rd13, [_Z12compute_gemmPK6__halfS1_PKfPfff_param_0];
ld.param.u64 %rd14, [_Z12compute_gemmPK6__halfS1_PKfPfff_param_1];
...
$L__BB0_2:
mov.u32 %r16727, %tid.x;
shr.u32 %r16726, %r16727, 5;
...
wmma.store.d.sync.aligned.row.m16n16k16.global.f32 [%rd54], {%f157, %f159, %f161, %f163, %f165, %f167, %f169, %f171}, %r18;

$L__BB1_20:
ret;

}

Nvidia Elf file
~~~~~~~~~~~~~~~

% readelf -S cudaTensorCoreGemm.simple
There are 35 section headers, starting at offset 0x260b78:

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
...
[18] .nv_fatbin        PROGBITS         0000000000092fa0  00092fa0
     0000000000172570  0000000000000000   A       0     0     8
[19] __nv_module_id    PROGBITS         0000000000205510  00205510
     000000000000000f  0000000000000000   A       0     0     8
...
  [29] .nvFatBinSegment  PROGBITS         000000000021f198  0021e198
       0000000000000030  0000000000000000  WA       0     0     8
...

% objdump -j __nv_module_id -s cudaTensorCoreGemm.opt

cudaTensorCoreGemm.opt:     file format elf64-x86-64

Contents of section __nv_module_id:
 205510 5f5f4e56 5f4d4f44 554c455f 494400    __NV_MODULE_ID. 

CUDA DNN
~~~~~~~~

Deep Neural Network library

% ls /usr/src/cudnn_samples_v8/
% ls /usr/include/cudnn*
/usr/include/cudnn_adv_infer.h  /usr/include/cudnn_cnn_infer.h  /usr/include/cudnn_ops_infer.h
/usr/include/cudnn_adv_train.h  /usr/include/cudnn_cnn_train.h  /usr/include/cudnn_ops_train.h
/usr/include/cudnn_backend.h    /usr/include/cudnn.h            /usr/include/cudnn_version.h
% ls /usr/lib/x86_64-linux-gnu/libcudnn.so
/usr/lib/x86_64-linux-gnu/libcudnn.so

Sample:
% pwd
/home/sheldon/cudnn_samples_v8/mnistCUDNN
% make all TARGET_ARCH=x86_64
% ./mnistCUDNN 
Executing: mnistCUDNN
cudnnGetVersion() : 8903 , CUDNN_VERSION from cudnn.h : 8903 (8.9.3)
Host compiler version : GCC 11.4.0

There are 1 CUDA capable devices on your machine :
device 0 : sms 16  Capabilities 7.5, SmClock 1785.0 Mhz, MemSize (Mb) 3903, MemClock 4001.0 Mhz, Ecc=0, boardGroupID=0
Using device 0

Testing single precision
...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!

Testing half precision (math in single precision)
...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!


CUDA BLAS
~~~~~~~~~

% ldd cublas_amin_example 
  linux-vdso.so.1 (0x00007fffb6de8000)
  libcublas.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublas.so.12 (0x00007ff1e0200000)
  libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff1dfe00000)
  libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff1e6b24000)
  libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff1dfa00000)
  /lib64/ld-linux-x86-64.so.2 (0x00007ff1e6c07000)
  libcublasLt.so.12 => /usr/local/cuda/targets/x86_64-linux/lib/libcublasLt.so.12 (0x00007ff1bd200000)
  librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff1e6b1d000)
  libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff1e6b18000)
  libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff1e6b13000)
  libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff1e6a2c000)

TensorRT
~~~~~~~~

% ls /usr/src/tensorrt/samples/

% cd sampleOnnxMNIST/; make
...
% ./sample_onnx_mnist --fp16
&&&& RUNNING TensorRT.sample_onnx_mnist [TensorRT v8601] # ./sample_onnx_mnist --fp16
[08/09/2023-21:58:34] [I] Building and running a GPU inference engine for Onnx MNIST
[08/09/2023-21:58:36] [I] [TRT] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 19, GPU 62 (MiB)
...
[08/09/2023-21:58:47] [I] Output:
[08/09/2023-21:58:47] [I]  Prob 0  0.0000 Class 0: 
[08/09/2023-21:58:47] [I]  Prob 1  0.0000 Class 1: 
[08/09/2023-21:58:47] [I]  Prob 2  0.0000 Class 2: 
[08/09/2023-21:58:47] [I]  Prob 3  0.0000 Class 3: 
[08/09/2023-21:58:47] [I]  Prob 4  0.9911 Class 4: **********
[08/09/2023-21:58:47] [I]  Prob 5  0.0001 Class 5: 
[08/09/2023-21:58:47] [I]  Prob 6  0.0000 Class 6: 
[08/09/2023-21:58:47] [I]  Prob 7  0.0000 Class 7: 
[08/09/2023-21:58:47] [I]  Prob 8  0.0000 Class 8: 
[08/09/2023-21:58:47] [I]  Prob 9  0.0088 Class 9: 
[08/09/2023-21:58:47] [I] 
&&&& PASSED TensorRT.sample_onnx_mnist [TensorRT v8601] # ./sample_onnx_mnist --fp16


% ./trtexec --explicitBatch --onnx=../data/mnist/mnist.onnx --workspace=1024
...
&&&& PASSED TensorRT.trtexec [TensorRT v8601] # ./trtexec --explicitBatch --onnx=../data/mnist/mnist.onnx --workspace=1024

